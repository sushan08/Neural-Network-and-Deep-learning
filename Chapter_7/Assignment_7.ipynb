{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eLM4qsm8Vr2"
      },
      "source": [
        "## Building word2vec model using Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJkrgByt8Vr3"
      },
      "source": [
        "Now that we have understood how word2vec model works, let us see how to build word2vec model using gensim library. Gensim is one of the popular scientific software packages widely used for building vector space models. It can be easily installed via pip. So, we can just type the following command in our terminal to install the gensim library:\n",
        "\n",
        "pip install -U gensim\n",
        "\n",
        "Now, we will learn how to build word2vec model using gensim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8dilzRO8Vr4"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#data processing\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "#modelling\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKzHS14C8Vr5"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "Load the dataset. The dataset used in this section is available in the data folder as text.zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2gYF8cV8Vr5"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('text.csv',header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw33-T2D8Vr5"
      },
      "source": [
        "Let us see what we got in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "3ggr3QuR8Vr5",
        "outputId": "53a4792f-52c4-4da9-8567-fa452c4c6ca7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>room kind clean strong smell dogs. generally a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stayed crown plaza april april . staff friendl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>booked hotel hotwire lowest price could find. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stayed husband sons way alaska cruise. loved h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>girlfriends stayed celebrate th birthdays. pla...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  room kind clean strong smell dogs. generally a...\n",
              "1  stayed crown plaza april april . staff friendl...\n",
              "2  booked hotel hotwire lowest price could find. ...\n",
              "3  stayed husband sons way alaska cruise. loved h...\n",
              "4  girlfriends stayed celebrate th birthdays. pla..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlVxU4Pj8Vr6"
      },
      "source": [
        "## Preprocess and prepare the dataset\n",
        "\n",
        "Define a function for preprocessing the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5JHPfsP8Vr6"
      },
      "outputs": [],
      "source": [
        "def pre_process(text):\n",
        "\n",
        "    #convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    #remove all special characters and keep only alpha numeric characters and spaces\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s.]',r'',text)\n",
        "\n",
        "    #remove new lines\n",
        "    text = re.sub(r'\\n',r' ',text)\n",
        "\n",
        "    # remove stop words\n",
        "    text = \" \".join([word for word in text.split() if word not in stopWords])\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa5sC-vk8Vr7"
      },
      "source": [
        "We will see how the preprocessed text looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZdzErbO8Vr7",
        "outputId": "ce25302e-752d-4f4c-d505-6b8aa5d782f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'agree fancy. everything needed. breakfast pool hot tub nice shuttle airport later checkout time. noise issue tough sleep through. awhile forget noisy door nearby noisy guests. complained management later email credit compd us amount requested would return.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_process(data[0][50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZglrOAb8Vr7"
      },
      "source": [
        "Preprocess the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czvTYNhz8Vr7"
      },
      "outputs": [],
      "source": [
        "data[0] = data[0].map(lambda x: pre_process(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2GA5Sj48Vr8"
      },
      "source": [
        "After preprocession our dataset looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Su41gS6P8Vr8",
        "outputId": "53d393a6-1a52-40b5-80dc-e695dfe8cffc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    room kind clean strong smell dogs. generally a...\n",
              "1    stayed crown plaza april april . staff friendl...\n",
              "2    booked hotel hotwire lowest price could find. ...\n",
              "3    stayed husband sons way alaska cruise. loved h...\n",
              "4    girlfriends stayed celebrate th birthdays. pla...\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ6N36cZ8Vr8"
      },
      "source": [
        "Genism library requires input in the from of list of lists. i.e,\n",
        "\n",
        "text = [ [word1, word2, word3], [word1, word2, word3] ]\n",
        "\n",
        "We know that each row in our data contains a set of sentences. So we split them by '.' and convert them into list i.e,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5YisRgP8Vr8",
        "outputId": "44cf2daf-dd6f-41d3-a01d-b73e7d9dcb53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['stayed crown plaza april april ',\n",
              " ' staff friendly attentive',\n",
              " ' elevators tiny ',\n",
              " ' food restaurant delicious priced little high side',\n",
              " ' course washington dc']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0][1].split('.')[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjXCQz8m8Vr8"
      },
      "source": [
        "Now, We have the data in a list. But we need to convert them into a list of lists. So, now again we split them by space ' '. i.e, First we split the data by '.' and then we split them by ' ' so that we can get our data in a list of lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK0XtPyg8Vr8"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for line in data[0][1].split('.'):\n",
        "    words = [x for x in line.split()]\n",
        "    corpus.append(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx3mmpw78Vr8"
      },
      "source": [
        "As you can see below, we have our inputs in the form of lists of lists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jisUhRd28Vr9",
        "outputId": "5a733730-c919-406e-eeeb-9c20067f1bcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['stayed', 'crown', 'plaza', 'april', 'april'],\n",
              " ['staff', 'friendly', 'attentive']]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFxHB66c8Vr9"
      },
      "source": [
        "Convert the whole text in our dataset to a list of lists and build a corpus. Corpus is just the collection of vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiO3iWQn8Vr9",
        "outputId": "34c49584-7ae7-482b-bb17-ae8f0bb31d47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['room', 'kind', 'clean', 'strong', 'smell', 'dogs'],\n",
              " ['generally', 'average', 'ok', 'overnight', 'stay', 'youre', 'fussy']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data[0].map(lambda x: x.split('.'))\n",
        "\n",
        "corpus = []\n",
        "for i in (range(len(data))):\n",
        "    for line in data[i]:\n",
        "        words = [x for x in line.split()]\n",
        "        corpus.append(words)\n",
        "\n",
        "corpus[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npw3nCPt8Vr9"
      },
      "source": [
        "Now the problem we have is our corpus contains only unigrams and it will not give us results when we give bigram as an input, for an example say 'san francisco'.\n",
        "\n",
        "So we use gensim's Phrases functions which collect all the words which occur together and add an underscore between them. So now 'san francisco' becomes 'san_francisco'. We set the min_count parameter to 25 which implies we ignore all the words and bigrams which appears lesser than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WgHeg8zp8Vr9"
      },
      "outputs": [],
      "source": [
        "phrases = Phrases(sentences=corpus,min_count=25,threshold=50)\n",
        "bigram = Phraser(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OG-t-8R8Vr9"
      },
      "outputs": [],
      "source": [
        "for index,sentence in enumerate(corpus):\n",
        "    corpus[index] = bigram[sentence]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu4Hsxc88Vr9"
      },
      "source": [
        "As you can see below underscore has been added to the bigrams in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1GtlpxH8Vr9",
        "outputId": "bdda5715-6823-40af-d714-0bdfb76c3c65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['connected', 'rivercenter', 'mall', 'downtown', 'san_antonio']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[111]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lea8xNcU8Vr9",
        "outputId": "364eba52-e846-43a3-f2a6-79e71c091db0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['course', 'washington_dc']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[9]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8vzp48d8Vr-"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAPoACMd8Vr-"
      },
      "source": [
        "Now let us build the model. Let us define some of the important hyperparameters that the model needs.\n",
        "\n",
        "\n",
        "* Size represents the size of the vector i.e dimensions of the vector to represent a word. The size can be chosen according to our data size. If our data is very small then we can set our size to a small value, but if we have significantly large dataset then we can set our vector size to 300. In our case, we set our size to 100\n",
        "\n",
        "* Window size represents the distance that should be considered between the target word and its neighboring word. Words exceeding the window size from the target word will not be considered for learning. Typically, a small window size is preferred.\n",
        "\n",
        "* Min count represents the minimum frequency of words. i.e if the particular word's occurrence is less than a min_count then we can simply ignore that word.\n",
        "\n",
        "* workers specify the number of worker threads we need to train the model\n",
        "\n",
        "* sg=1 implies we use skip-gram method for training if sg=0 then it implies we use CBOW for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuhDOZ0L8Vr-"
      },
      "outputs": [],
      "source": [
        "size = 100\n",
        "window_size = 2\n",
        "epochs = 100\n",
        "min_count = 2\n",
        "workers = 4\n",
        "sg = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCJtMPQz8Vr-"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TByFKHns8Vr-"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(corpus,sg=1,window=window_size,vector_size=size, min_count=min_count,workers=workers,epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEQXmxR18Vr-"
      },
      "source": [
        "To save and load the model, we can simply use save and load functions respectivley.\n",
        "\n",
        "Save the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZnM6md08Vr-"
      },
      "outputs": [],
      "source": [
        "model.save('word2vec.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWIArX5x8Vr-"
      },
      "source": [
        "Load the saved word2vec model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvOJyrWZ8Vr-"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load('word2vec.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irFkSe5t8Vr-"
      },
      "source": [
        "## Evaluate the Embeddings\n",
        "\n",
        "After training the model, we evaluate them. Let us see what the model has been learned and how well it has understood the semantics of words. Genism provides a most_similar function which gives us top similar words related to the given word.\n",
        "\n",
        "As you can see below, given san_deigo as an input we are getting all other related city names as most similar words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmJZfHub8VsC",
        "outputId": "ca8260fe-ade7-4cee-c514-36a18f16e087"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('san_antonio', 0.7965883016586304),\n",
              " ('austin', 0.7555802464485168),\n",
              " ('san_francisco', 0.7504624128341675),\n",
              " ('boston', 0.7418094277381897),\n",
              " ('dallas', 0.7365104556083679),\n",
              " ('indianapolis', 0.7337252497673035),\n",
              " ('memphis', 0.7308087944984436),\n",
              " ('seattle', 0.73016756772995),\n",
              " ('la', 0.7283918857574463),\n",
              " ('phoenix', 0.7246084809303284)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar('san_diego')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGlum8Pc8VsC"
      },
      "source": [
        "We can also apply arithmetic operations on our vector to check how accurate our vectors are, For instance, woman + king - man = queen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DCo_wJ78VsC",
        "outputId": "9b6c6bad-43de-4f5e-9424-fc28b945da6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.7528093457221985)]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnc-nSZB8VsC"
      },
      "source": [
        "We can also find the words that do not match in the given set of words, for instance in the below list called text except the word holiday all others are city names and since our word2vec has understood the semantics of each word it returns the word holiday as the one that does not match with the other words in the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JMsgSjL8VsD",
        "outputId": "4014ad30-4765-414c-fddf-87ab28c34272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'holiday'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = ['los_angeles','indianapolis', 'holiday', 'san_antonio','new_york']\n",
        "\n",
        "model.wv.doesnt_match(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s76k3r2Y8VsD"
      },
      "source": [
        "Thus, with word2vec model, we can generate useful word embeddings which captures the syntactic and semantic meanings of the word. In the next section, we will learn how to visualize this word embeddings generated by the word2vec model in TensorBoard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYI2MHTD8VsD"
      },
      "source": [
        "## Visualizing Word Embeddings in TensorBoard\n",
        "\n",
        "\n",
        "\n",
        "In the last section, we learned how to build word2vec model for generating word embeddings using gensim.\n",
        "Now, we will see how to visualize those embeddings using TensorBoard. Visualizing word embeddings help us to understand the projection space and also helps us to easily validate the embeddings. TensorBoard provides us a built-in visualizer called the embedding projector for interactively visualizing and analyzing the high-dimensional data like our word embeddings. We will learn how can we use the tensorboard's projector for visualizing the word embeddings step by step.\n",
        "\n",
        "\n",
        "Import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXZLerH_8VsD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gensim\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VEkLwiL8VsD"
      },
      "source": [
        "Load the saved model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOM5AByN8VsD"
      },
      "outputs": [],
      "source": [
        "file_name = \"word2vec.model\"\n",
        "model = gensim.models.keyedvectors.KeyedVectors.load(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLPGx29e8VsD"
      },
      "source": [
        "Once after loading the model, we will save length of the vocaublary (number of words in our vocabulary) into a variable called max_size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sil-qQvH8VsD"
      },
      "outputs": [],
      "source": [
        "max_size = len(model.wv.index_to_key)-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBlWNmg68VsD"
      },
      "source": [
        "We learned that the dimension of word vectors will be $ V \\times N$. That is, Length of the vocabulary ($V$) $\\times$ Number of neurons in the hidden layer ($N$). So, we initialize a matrix named  w2v with the shape as max_size which is the vocabulary size and the model's first layer size which is the number of neurons in the hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0YPtCpp8VsD"
      },
      "outputs": [],
      "source": [
        "w2v = np.zeros((max_size,model.layer1_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iWSnGs_8VsD"
      },
      "source": [
        "Now we create a new file called metadata.tsv where we save all the words in our model and we also store the embedding of each word in the w2v matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br5DO-LE8VsD"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('projections'):\n",
        "    os.makedirs('projections')\n",
        "\n",
        "with open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n",
        "\n",
        "    for i, word in enumerate(model.wv.index_to_key[:max_size]):\n",
        "\n",
        "        #store the embeddings of the word\n",
        "        w2v[i] = model.wv[word]\n",
        "\n",
        "        #write the word to a file\n",
        "        file_metadata.write(word + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySiA7oYW8VsE"
      },
      "source": [
        "Next, we initialize the tensorflow session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6aDS6FW8VsE"
      },
      "outputs": [],
      "source": [
        "sess = tf.InteractiveSession()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0yXWuwh8VsE"
      },
      "source": [
        "Initialize the tensorflow variable called embeddings that holds the word embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x3FWyQg8VsE"
      },
      "outputs": [],
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "    embedding = tf.Variable(w2v, trainable=False, name='embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088Gufhj8VsE"
      },
      "source": [
        "Initialize all variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgf5bD1V8VsE"
      },
      "outputs": [],
      "source": [
        "tf.global_variables_initializer().run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9y6BZJx8VsE"
      },
      "source": [
        "Create an object to the saver class which is actually used for saving and restoring variables to and from our checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2TK6H2j8VsE"
      },
      "outputs": [],
      "source": [
        "saver = tf.train.Saver()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEQUGDIp8VsE"
      },
      "source": [
        "Using FileWriter, we save our summaries and events to our event file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCW1DEIn8VsE"
      },
      "outputs": [],
      "source": [
        "writer = tf.summary.FileWriter('projections', sess.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhzcWa7e8VsE"
      },
      "source": [
        "Initialize the projectors and add the embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdyDprcA8VsF"
      },
      "outputs": [],
      "source": [
        "config = projector.ProjectorConfig()\n",
        "embed= config.embeddings.add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyzxatvD8VsF"
      },
      "source": [
        "Next, we specify our tensor_name as embedding and metadata_path to the metadata.tsv file where we have the words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so-OLLXY8VsF"
      },
      "outputs": [],
      "source": [
        "embed.tensor_name = 'embedding'\n",
        "embed.metadata_path = 'metadata.tsv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdVOIM7k8VsF"
      },
      "source": [
        "And finally, save the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "v-rsklcO8VsF",
        "outputId": "efae7138-9b4a-4f63-dea3-afd85f8c23cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'projections/model.ckpt-28070'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "projector.visualize_embeddings(writer, config)\n",
        "\n",
        "saver.save(sess, 'projections/model.ckpt', global_step=max_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jTm7ZAv28VsF"
      },
      "source": [
        "Now, open the terminal and type the following command to open the tensorboard,\n",
        "\n",
        "tensorboard --logdir=projections --port=8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOmWnd1-8VsF"
      },
      "source": [
        "Thus, visualizing word embeddings in TensorBoard helps us to easily validate them. In the next section, We will how to convert paragraphs/documents to vectors using two different algorithms called PV-DM and PV-DBOW."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuyvIGh28VsF"
      },
      "source": [
        "# Finding similar documents using Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pOJrgPVS8VsF"
      },
      "source": [
        "We just learned how PV-DM and PV-DBOW convert the documents to a vector. Now, we will see how to perform document classification using Doc2Vec.\n",
        "\n",
        "In this section, we will use the 20 newsgroups dataset. It consists of 20,000 documents over 20 different\n",
        "news categories. We will use only four categories: Computer, Politics, Science, and Sports. We have 1000 documents under each of these four categories.\n",
        "\n",
        "We rename the documents with a prefix, category_. For example, all science documents are renamed as\n",
        "Science_1, Science_2, and so on. After renaming them, we combine all the documents and\n",
        "place them in a single folder. The combined data is available in the data folder has new_dataset.zip.\n",
        "\n",
        "\n",
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFL5NHxq8VsF"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stopWords = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksdV3BDK8VsF"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqa0Htw88VsG"
      },
      "source": [
        "Load all the documents and save the document names in docLabels list and document content in a list called data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX9lSso_8VsG"
      },
      "outputs": [],
      "source": [
        "docLabels = []\n",
        "docLabels = [f for f in os.listdir('data/news_dataset') if  f.endswith('.txt')]\n",
        "\n",
        "data = []\n",
        "for doc in docLabels:\n",
        "      data.append(open('data/news_dataset/'+doc).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "t81ugfR-8VsG"
      },
      "source": [
        "As shown below, docLabels has names of our documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfCNa8PT8VsG",
        "outputId": "a6525406-dc29-4401-ddbd-8df27db29921"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Electronics_827.txt',\n",
              " 'Electronics_848.txt',\n",
              " 'Science_377.txt',\n",
              " 'Science_24.txt',\n",
              " 'Politics_38.txt']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docLabels[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eitAeC-s8VsG"
      },
      "source": [
        "Define a class called DocIterator which acts as an iterator to runs over all the documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BTmNJ_f8VsG"
      },
      "outputs": [],
      "source": [
        "class DocIterator(object):\n",
        "    def __init__(self, doc_list, labels_list):\n",
        "        self.labels_list = labels_list\n",
        "        self.doc_list = doc_list\n",
        "\n",
        "    def __iter__(self):\n",
        "        for idx, doc in enumerate(self.doc_list):\n",
        "            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgShzbqX8VsG"
      },
      "source": [
        "Create an object called 'it' to the DocIterator class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2i-xEVX8VsG"
      },
      "outputs": [],
      "source": [
        "it = DocIterator(data, docLabels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wsdQYVF8VsG"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Now let us build the model. Let us define some of the important hyperparameters of the model.\n",
        "\n",
        "* Size represents our embedding size.\n",
        "\n",
        "* alpha represents our learning rate.\n",
        "\n",
        "* min_alpha implies that our learning rate alpha will decay to min_alpha during training.\n",
        "\n",
        "* dm=1 implies we use ‘distributed memory’ (PV-DM) and if we set dm =0 it implies we use ‘distributed bag of words’ (PV-DBOW) for training.\n",
        "\n",
        "* min_count represents the minimum frequcy of words. i.e if the paritcular word's occrurence is less than a min_count than we can simply ignore that word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQpqJioq8VsH"
      },
      "outputs": [],
      "source": [
        "size = 100\n",
        "alpha = 0.025\n",
        "min_alpha = 0.025\n",
        "dm = 1\n",
        "min_count = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQxnLBXs8VsH"
      },
      "source": [
        "Define the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epRCmZfQ8VsH"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Doc2Vec(size=size, min_count=min_count, alpha=alpha, min_alpha=min_alpha, dm=dm)\n",
        "model.build_vocab(it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJxMZtLm8VsH"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5icRfSo8VsH"
      },
      "outputs": [],
      "source": [
        "for epoch in range(100):\n",
        "    model.train(it,total_examples=120,epochs = model.iter)\n",
        "    model.alpha -= 0.002\n",
        "    model.min_alpha = model.alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W_D_-Cg8VsH"
      },
      "source": [
        "Save the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewv32pNN8VsH"
      },
      "outputs": [],
      "source": [
        "model.save('model/doc2vec.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPA3sql88VsH"
      },
      "source": [
        "\n",
        "We can load the saved model using load function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9rnMapi8VsH"
      },
      "outputs": [],
      "source": [
        "d2v_model = gensim.models.doc2vec.Doc2Vec.load('model/doc2vec.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_1x-HoE8VsH"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEQBF3aK8VsH"
      },
      "source": [
        "After training, we evaluate the model performance. As shown below, when we feed Electronics_724.txt document as an input, it returns all the related documents with their corresponding scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS52h3aM8VsI",
        "outputId": "9a11f44f-7f33-4570-f861-c4d7d11a46b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Electronics_407.txt', 0.9127770662307739),\n",
              " ('Electronics_163.txt', 0.8796253800392151),\n",
              " ('Science_480.txt', 0.8787260055541992),\n",
              " ('Science_769.txt', 0.8782669305801392),\n",
              " ('Science_627.txt', 0.8712874054908752),\n",
              " ('Science_737.txt', 0.8702232241630554),\n",
              " ('Electronics_461.txt', 0.8684250116348267),\n",
              " ('Science_377.txt', 0.8677175045013428),\n",
              " ('Electronics_786.txt', 0.867066502571106),\n",
              " ('Politics_167.txt', 0.8663994669914246)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.docvecs.most_similar('Electronics_724.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTwBSt6U8VsI"
      },
      "source": [
        "We learned how to generate embeddings for the documents using doc2vec algorithms, in the next section, we will learn how to generate sentence embeddings using skip-thoughts and quick-thoughts algorithms."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}